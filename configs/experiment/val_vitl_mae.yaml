# @package _global_

# to execute this experiment run:
# python train.py experiment=val_vit_mae.yaml

defaults:
  - override /data: kinetics
  - override /model: vit_mae
  - override /callbacks: default
  - override /trainer: default
  # - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["kinetics", "vit_mae_val_singlecrop"]

seed: 42

task_name: "vitl_test"
devices: 4
batch_size: 32

trainer:
  precision: 16
  devices: ${devices}
  accelerator: "gpu"
  min_epochs: 10
  max_epochs: 10
  gradient_clip_val: 0.5

model:
  compile: true
  pretrain: "checkpoints/vitl_mae.pth"
  tokenizer_cfg:
    _target_: src.models.tokenizer_utils.TokenizerConfig
    drop_policy: none
    drop_param: 0.1
    encode_length: False
  model:
    embed_dims: 1024
    depth: 24
    num_heads: 16
    use_flash_attn: true


data:
  batch_size: ${batch_size}
  

# logger: 
#   wandb:
#     project: "pregrouping"
#     tags: ${task_name}
#     group: "baselines"